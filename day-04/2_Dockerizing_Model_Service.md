# 2. Dockerizing Model Service

## ðŸŽ¯ Learning Objectives
- Learn how to package the FastAPI ML model service into a Docker container.  
- Understand how to build and run the container locally.  
- Prepare the image for deployment with Docker Compose.  

---

## ðŸ“˜ Why Dockerize the Model?

- Guarantees **consistent environment** across machines.  
- Simplifies **deployment** (server, cloud, or on-prem).  
- Enables integration with **Docker Compose** for multi-service orchestration.  

---

## ðŸ›  Step 1: Project Structure

```
.
â”œâ”€â”€ app/
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ models/
â”‚   â””â”€â”€ catdog_model.pth
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile.serve
```

`requirements.txt`:
```
fastapi
uvicorn[standard]
pydantic
torch
torchvision
Pillow
```

---

## ðŸ›  Step 2: Dockerfile for Serving

`Dockerfile.serve`:

```dockerfile
FROM python:3.10-slim

# Install dependencies
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy code and model
COPY app/ ./app/
COPY models/ ./models/

# Expose port
EXPOSE 8000

# Run FastAPI app
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

---

## ðŸ›  Step 3: Build the Image

```bash
docker build -f Dockerfile.serve -t catdog-api:latest .
```

Check images:
```bash
docker images
```

---

## ðŸ›  Step 4: Run the Container

```bash
docker run --rm -it -p 8000:8000 catdog-api:latest
```

Now test the API at ðŸ‘‰ http://localhost:8000/docs  

---

## ðŸ›  Step 5: Mount Models (Optional)

If you want to update models without rebuilding the image, mount a volume:

```bash
docker run --rm -it -p 8000:8000   -v $(pwd)/models:/app/models   catdog-api:latest
```

---

## ðŸ§© Step 6: Push to Registry (Optional)

```bash
docker tag catdog-api:latest ghcr.io/<username>/catdog-api:latest
docker push ghcr.io/<username>/catdog-api:latest
```

---

## âœ… Summary
- You created a `Dockerfile.serve` to containerize the FastAPI model API.  
- Built and ran the container locally.  
- Prepared it for deployment via **Docker Compose**.  